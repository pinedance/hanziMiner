{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import namedtuple\n",
    "from collections import Counter\n",
    "import math\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "import sys\n",
    "\n",
    "class Tools:\n",
    "    # Frome here : https://github.com/lovit/soynlp/tree/master/soynlp/utils\n",
    "    @staticmethod\n",
    "    def get_available_memory():\n",
    "        \"\"\"It returns remained memory as percentage\"\"\"\n",
    "\n",
    "        mem = psutil.virtual_memory()\n",
    "        return 100 * mem.available / (mem.total)\n",
    "\n",
    "    def get_process_memory():\n",
    "        \"\"\"It returns the memory usage of current process\"\"\"\n",
    "\n",
    "        process = psutil.Process(os.getpid())\n",
    "        return process.memory_info().rss / (1024 ** 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KMcorpus\n",
    "\n",
    "class KMcorpus:\n",
    "    \n",
    "    PC = \"[,\\.!\\?！＇，ㆍ．／：；？｀、。·‥…¨〃∼´～˝\\(\\)\\{\\}\\[\\]（）［］｛｝‘’“”〔〕〈〉《》「」『』【】]\"\n",
    "    \n",
    "    def __init__( self, text, comments_header=\"#\", doc_sep=\"\\r?\\n\\r?\\n\" ):\n",
    "        self.text = text\n",
    "        self.doc_sep = doc_sep\n",
    "        self.comments_header = comments_header\n",
    "        self.docs = []\n",
    "        \n",
    "    def remove_comments( self ):\n",
    "        pattern = \"{}.*?$\".format( self.comments_header )\n",
    "        regex = re.compile( pattern, re.MULTILINE|re.DOTALL )\n",
    "        self.text = re.sub( regex, \"\", self.text ).strip()\n",
    "        print(\"# Comments were removed\")\n",
    "        return self\n",
    "        \n",
    "    def remove_punctuation( self ):  \n",
    "        regex_PC = re.compile( self.PC )\n",
    "        self.text = re.sub( regex_PC , \" \", self.text ).strip()\n",
    "        print(\"# Punctuations were removed\")\n",
    "        return self\n",
    "    \n",
    "    def remove_chrs( self, chr_types=[\"Korean\", \"Alphabet\", \"Numbers\"] ):\n",
    "        if \"Korean\" in chr_types:\n",
    "            self.text = re.sub( re.compile(\"[가-힣]\"), \"\", self.text )\n",
    "        if \"Alphabet\" in chr_types:\n",
    "            self.text = re.sub( re.compile(\"[a-zA-Z]\"), \"\", self.text )\n",
    "        if \"Numbers\" in chr_types:\n",
    "            self.text = re.sub( re.compile(\"[\\d]+?\"), \"\", self.text )\n",
    "        self.text = self.text.strip()\n",
    "        print(\"# {} were removed\".format( \", \".join( chr_types  ) ) )\n",
    "        return self\n",
    "    \n",
    "    def merge_spaces( self ):\n",
    "        self.text = re.sub( re.compile(\"[ \\t]+?\"), \" \", self.text )\n",
    "        self.text = re.sub( re.compile(\"^[ \\t]+?\", re.MULTILINE|re.DOTALL), \"\", self.text ).strip()\n",
    "        print(\"# Spaces were merged\")\n",
    "        return self\n",
    "        \n",
    "    def text2docs(self):\n",
    "        docs = re.split( re.compile( self.doc_sep ), self.text )\n",
    "        self.docs = [ doc.strip().split() for doc in docs ]\n",
    "        print(\"# Text was converted to List Data\")\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def merge_duplications(self, dict_path=\"dicts/duplications.dic\" ):\n",
    "        print(\"# Duplicated Characters were merged\")\n",
    "        self.text = self.__class__.merge_chrs( self.text, dict_path ) \n",
    "        return self\n",
    "    \n",
    "    def merge_variants(self, dict_path=\"dicts/variants.dic\" ):\n",
    "        print(\"# Variants Characters were merged\")\n",
    "        self.text = self.__class__.merge_chrs( self.text, dict_path ) \n",
    "        return self\n",
    "    \n",
    "\n",
    "    \n",
    "    @staticmethod\n",
    "    def merge_chrs(text, dict_path ):\n",
    "        dic = open(dict_path, 'r', encoding='utf-8').readlines()\n",
    "        text_ = text + \"\"\n",
    "        for pair in dic:\n",
    "            a, b = pair.split(\"\\t\")\n",
    "            text_ = text_.replace(a, b)\n",
    "        return text_\n",
    "    \n",
    "    def ngram( text, n):\n",
    "        return [ text[i:i+n] for i in range( 0, len(text) - n + 1 )  ]\n",
    "\n",
    "    def allgram( text, min_window=2, max_window=8 ):\n",
    "        len_txt = len(text)\n",
    "        mx_wd = len_txt if ( len_txt < max_window ) else max_window\n",
    "        rst = []\n",
    "        for i in range(min_window, mx_wd + 1):\n",
    "            rst += KMcorpus.ngram(text, i)\n",
    "        return rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segment\n",
    "\n",
    "class TokenExtractor:\n",
    "    \n",
    "    def __init__( self, corpus ):\n",
    "        self.corpus = corpus\n",
    "        self.token_counter = Counter()\n",
    "        self.unigram_counter = Counter( self.corpus.text )\n",
    "        self.bigram_counter = Counter()\n",
    "        \n",
    "    def _cohesion_score( self, word ):\n",
    "        word_len = len( word )\n",
    "        if (not word) or ( word_len < self.min_window ):\n",
    "            return 0\n",
    "        \n",
    "        first_chr_freq = self.unigram_counter[ word[0] ]\n",
    "        last_chr_freq = self.unigram_counter[ word[-1] ]\n",
    "        whole_word_freq = self.token_counter[ word ]\n",
    "        \n",
    "        cohesion_l = 0 if whole_word_freq == 0 else math.pow( ( whole_word_freq / first_chr_freq ), (1 / (word_len - 1)) )\n",
    "        cohesion_r = 0 if whole_word_freq == 0 else math.pow( ( whole_word_freq / last_chr_freq ), (1 / (word_len - 1)) )\n",
    "        cohesion = math.sqrt(cohesion_l * cohesion_r)\n",
    "        return ( cohesion_l, cohesion_r, cohesion , (cohesion_l + cohesion_r)/2 )\n",
    "\n",
    "    def _branch_entropy_score( self, word ):\n",
    "        word_len = len( word )\n",
    "        whole_word_freq = self.token_counter[ word ]\n",
    "        token_l, token_r = word[:-1], word[1:]        \n",
    "        branch_entropy_l = self.__class__.entropy( whole_word_freq / self.token_counter[token_l] ) if ( token_l in self.token_counter ) and (self.token_counter[token_l] != 0 ) else 0\n",
    "        branch_entropy_r = self.__class__.entropy( whole_word_freq / self.token_counter[token_r] ) if ( token_r in self.token_counter ) and (self.token_counter[token_r] != 0 ) else 0\n",
    "        \n",
    "        # debuging ###\n",
    "        if not( ( token_l in self.token_counter ) and (self.token_counter[token_l] != 0 ) ):\n",
    "            print(\"token_l\", word, token_l, self.token_counter[token_l] )\n",
    "            \n",
    "        if not ( ( token_r in self.token_counter ) and (self.token_counter[token_r] != 0 ) ):\n",
    "            print(\"token_r\", word, token_r, self.token_counter[token_r] )\n",
    "        ### \n",
    "        \n",
    "        return ( ( token_l, branch_entropy_l ), ( token_r, branch_entropy_r ) )\n",
    "\n",
    "   # return self\n",
    "        \n",
    "    def train( self, min_freq = 5, min_window=2, max_window=8  ):\n",
    "        self.min_freq = min_freq\n",
    "        self.max_window = max_window\n",
    "        if min_window < 2:\n",
    "            self.min_window = 2\n",
    "            print(\"!!! Min_window must be greater than 2. Automatically set 2\")\n",
    "        else:\n",
    "            self.min_window = min_window\n",
    "        \n",
    "        corpus_size = len(self.corpus.docs)\n",
    "        \n",
    "        for i, doc in enumerate(self.corpus.docs):\n",
    "            sys.stdout.write(\"\\r# Training ... ({:06d} in {:06d} docs) System memory {:.3f} Gb used\".format(i, corpus_size, Tools.get_process_memory() ) )\n",
    "            for phrase in doc:\n",
    "                particles = KMcorpus.allgram( phrase, min_window, max_window + 1 ) # branch entropy를 구히기 위해 window 범위를 1씩 늘림\n",
    "                self.token_counter.update( Counter( particles ) )\n",
    "\n",
    "                bigrams = KMcorpus.ngram( phrase, n=2 )\n",
    "                self.bigram_counter.update( Counter( bigrams ) )\n",
    "\n",
    "        # Token_Counter\n",
    "        self.token_counter.update( self.unigram_counter ) # branch entropy를 구히기 위해 window 범위를 1씩 늘림\n",
    "        # Branch Entropy\n",
    "        self._total_branch_entropy_score()\n",
    "        print( \"\\r# Training was done. Used memory {:.3f} Gb\".format( Tools.get_process_memory() ) )\n",
    "        return self\n",
    "        \n",
    "    def _total_branch_entropy_score( self ):\n",
    "        branch_entropy_l = defaultdict(lambda: 0)\n",
    "        branch_entropy_r = defaultdict(lambda: 0)\n",
    "        for (w, f) in self.token_counter.items():\n",
    "            if ( len(w) < self.min_window ): continue\n",
    "            be_l, be_r = self._branch_entropy_score( w )\n",
    "            branch_entropy_l[ be_l[0] ] += be_l[1]\n",
    "            branch_entropy_r[ be_r[0] ] += be_r[1]\n",
    "        self.total_branch_entropy_l = branch_entropy_l\n",
    "        self.total_branch_entropy_r = branch_entropy_r\n",
    "        return self\n",
    "    \n",
    "    def extract( self ):\n",
    "        self._score = defaultdict()\n",
    "        self._score_header = ['freq', 'cohesion_l', 'cohesion_r', 'cohesion', 'cohesion_s', 'branch_entropy_l', 'branch_entropy_r', 'branch_entropy' ]\n",
    "        i = 0\n",
    "        for (w, f) in self.token_counter.items():\n",
    "            if len(w) > self.max_window: continue\n",
    "            if len(w) < self.min_window: continue\n",
    "            if f < self.min_freq: continue\n",
    "\n",
    "            _score = namedtuple('Score', self._score_header )\n",
    "            _score.freq = f\n",
    "            # Cohesion Score\n",
    "            _score.cohesion_l, _score.cohesion_r, _score.cohesion, _score.cohesion_s  = self._cohesion_score( w )\n",
    "            # Branch Entropy Score\n",
    "            _score.branch_entropy_l = self.total_branch_entropy_l[w]\n",
    "            _score.branch_entropy_r = self.total_branch_entropy_r[w]\n",
    "            _score.branch_entropy = ( _score.branch_entropy_l + _score.branch_entropy_r ) / 2\n",
    "            self._score[ w ] = _score\n",
    "            \n",
    "            # Report progress\n",
    "            i += 1\n",
    "            sys.stdout.write(\"\\r# Extracting ... ({:08d} ) System memory {:.3f} Gb used\".format(i, Tools.get_process_memory() ) )\n",
    "\n",
    "        print(\"# Extrating was done. System memory {:.3f} Gb used\".format( Tools.get_process_memory()) )\n",
    "        return self\n",
    "    \n",
    "    # get score\n",
    "    def score(self):\n",
    "        return self._score\n",
    "    \n",
    "    def report(self, output_filename, sep=\"\\t\", order=\"cohesion\"):\n",
    "        handler = open(output_filename, 'w', encoding=\"utf-8\")\n",
    "        header = \"token\" + sep + sep.join( self._score_header ) + \"\\n\"\n",
    "        handler.write(header)\n",
    "\n",
    "        _score_list = self.score().items()\n",
    "        score_list = sorted( _score_list, key=lambda x: getattr( x[1], order ), reverse=True )\n",
    "        for word, score in score_list:\n",
    "            handler.write( word + sep + sep.join( [ \"{:01.3f}\".format( getattr( score, s ) ) for s in self._score_header ]) + \"\\n\" )\n",
    "        handler.close()\n",
    "        print(\"# {:d} of tokens were reported in {}\".format( len( score_list) , output_filename  ) )\n",
    "        return self\n",
    "                          \n",
    "    @staticmethod\n",
    "    def entropy( p ):\n",
    "        return -1 * p * math.log2( p )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Segmenter:\n",
    "    \n",
    "    def __init__( self, token_with_score, target_score=\"cohesion_s\", score_cutoff=0):\n",
    "        self.score_list = [ ( tk, getattr( sc, target_score ) ) for tk, sc in token_with_score.items() if getattr( sc, target_score ) >= score_cutoff  ]\n",
    "        self.score = dict( self.score_list )\n",
    "        self.tokens = self.score.keys()\n",
    "        self.target_text = \"\"\n",
    "        \n",
    "    def load( self, text, min_window=2, max_window=8 ):\n",
    "        _token_candis = set( KMcorpus.allgram(text, min_window, max_window) )\n",
    "        token_candis_with_score = [ ( it, self.score[it] ) for it in _token_candis if it in self.tokens ]\n",
    "        self.token_candis = sorted( token_candis_with_score, key=lambda x: (-x[1], -len(x[0] )  ) )\n",
    "        self.target_text = text\n",
    "        return self\n",
    "    \n",
    "    def segment( self, segment_marker=\"$\" ):\n",
    "        target_text = self.target_text + \"\"\n",
    "        self.segment_marker = segment_marker\n",
    "        for i, candi in enumerate( self.token_candis ):\n",
    "            marker = \"{0}{1}{0}\".format( self.segment_marker, i )\n",
    "            target_text = marker.join( target_text.split( candi[0] ) )\n",
    "        \n",
    "        self.text_segment_marked = target_text\n",
    "        return self\n",
    "    \n",
    "    def show( self, verbose=False, sep=\"%\"):\n",
    "        target_text = self.text_segment_marked + \"\"\n",
    "        for i, candi in enumerate( self.token_candis ):\n",
    "            marker = \"{0}{1}{0}\".format( self.segment_marker, i )\n",
    "            seg = \"【{0}/{1:01.3f}】\".format( candi[0], candi[1] ) if verbose else \"【{}】\".format( candi[0] )\n",
    "            target_text = target_text.replace(marker, seg )\n",
    "        self.text_segmented = target_text    \n",
    "        return target_text\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Duplicated Characters were merged\n",
      "# Variants Characters were merged\n",
      "# Comments were removed\n",
      "# Punctuations were removed\n",
      "# Korean, Alphabet, Numbers were removed\n",
      "# Spaces were merged\n",
      "# Text was converted to List Data\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "text = open(\"_tmp/_dummy_corpus.txt\", 'r', encoding=\"utf-8\").read()\n",
    "corpus = KMcorpus(text, comments_header=\"//\")\n",
    "corpus.merge_duplications().merge_variants().remove_comments().remove_punctuation().remove_chrs().merge_spaces().text2docs()\n",
    "\n",
    "tmp = open(\"_tmp/_dummy_corpus_clean.txt\", 'w', encoding=\"utf-8\")\n",
    "pp = pprint.PrettyPrinter(indent=4, stream=tmp)\n",
    "pp.pprint( corpus.docs )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training was done. Used memory 1.660 Gbstem memory 1.512 Gb used\n",
      "# Extracting ... (00036585 ) System memory 1.099 Gb used# Extrating was done. System memory 1.099 Gb used\n",
      "# 36585 of tokens were reported in _tmp/_dummy_corpus_tokens.txt\n",
      "train ending\n"
     ]
    }
   ],
   "source": [
    "te = TokenExtractor( corpus )\n",
    "te.train().extract().report(\"_tmp/_dummy_corpus_tokens.txt\")\n",
    "print(\"train ending\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sg = Segmenter( te.score(), target_score='cohesion' )\n",
    "# sgl = Segmenter( te.score, target_score='cohesion_l' )\n",
    "# sgr = Segmenter( te.score, target_score='cohesion_r' )\n",
    "sgE = Segmenter( te.score(), target_score='branch_entropy' )\n",
    "\n",
    "\n",
    "docs = [\n",
    "    \"治風證眩暈. 山茱萸肉 一兩, 山藥ㆍ甘菊ㆍ人參ㆍ川芎ㆍ茯神 各五錢. 右爲末, 每二錢, 酒調下. 《本事》\",\n",
    "    \"眞人養生銘曰人欲勞於形百病不能成飮酒勿大醉諸疾自不生食了行百步數以手摩肚寅丑日剪甲頭髮梳百度飽卽立小便飢則坐漩尿行處勿當風居止無小隙常夜濯足臥飽食終無益思慮最傷神喜怒最傷氣每去鼻中毛常習不唾地平明欲起時下床先左脚一日無災殃去邪兼辟惡如能七星步令人長壽樂酸味傷於筋苦味傷於骨甘卽不益肉辛多敗正氣鹹多促人壽不得偏耽嗜春夏少施泄秋冬固陽事獨臥是守眞愼靜最爲貴錢財生有分知足將爲利强知是大患少慾終無累神靜自常安修道宜終始書之屋壁中將以傳君子\",\n",
    "    \"久服明目輕身延年酒浸曝乾蒸之如此九次搗爲末每二錢空心溫酒調服一日二次本草\",\n",
    "    \"治折傷後爲風寒濕所侵手足疼痛生蒼朮破古紙半生半炒骨碎補穿山甲桑灰炒爲珠生草烏各二兩茴香一兩半右將草烏剉如麥大同連皮生薑四兩擂爛淹兩宿焙乾同前藥爲末酒糊和丸梧子大溫酒下五十丸少麻無妨得效\"\n",
    "]\n",
    "\n",
    "for sn in docs:\n",
    "    sg.load( sn ).segment()\n",
    "    #sgl.load( sn ).segment()\n",
    "    #sgr.load( sn ).segment()\n",
    "    sgE.load( sn ).segment()\n",
    "#     print( sgs.token_candis )\n",
    "    #print( sg.target_text )\n",
    "    print( sg.show() )\n",
    "    #print( sgl.show() )\n",
    "    #print( sgr.show() )\n",
    "    print( sgE.show() )\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SANDBOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.83666002653407556"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.power( 0.7, 1/2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ending\n"
     ]
    }
   ],
   "source": [
    "rst = sorted( te.score.items(), key=lambda it: -1* ( it[1].cohesion_r * it[1].cohesion_l ) )\n",
    "tmp2 = open(\"_tmp/_dummy_corpus_score.txt\", 'w', encoding=\"utf-8\")\n",
    "pp = pprint.PrettyPrinter(indent=4, stream=tmp2)\n",
    "pp.pprint( [ ( r[0], r[1].freq , r[1].cohesion_r, r[1].cohesion_l ) for r in rst ] )\n",
    "\n",
    "print(\"ending\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "te.unigram_counter.get(\"枸\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'i': 17, 'h': 5, 'w': 4, 'u': 3, 'e': 2, 'r': 2, 'q': 2, ' ': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(\"huihihu ihuihi\") \n",
    "d = Counter(\"werwwqweqriiiiiiiiiiii\")\n",
    "c.update(d)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.125"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "97 / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"c\" in {'a':1, 'b':2}.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': 1, 'b': 2}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict( [('a', 1), ('b', 2)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-664.3856189774724"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "def entropy( p ):\n",
    "    return -1 * p * math.log2( p )\n",
    "entropy( 100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'23456789'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"123456789\"[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "training ... (100 in 100 sents) use memory 1.143 Gb\r",
      "training ... (100 in 100 sents) use memory 1.143 Gb\n",
      "\r",
      "training was done. used memory 1.143 Gb\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import psutil\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_available_memory():\n",
    "    \"\"\"It returns remained memory as percentage\"\"\"\n",
    "\n",
    "    mem = psutil.virtual_memory()\n",
    "    return 100 * mem.available / (mem.total)\n",
    "\n",
    "def get_process_memory():\n",
    "    \"\"\"It returns the memory usage of current process\"\"\"\n",
    "    \n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / (1024 ** 3)\n",
    "\n",
    "\n",
    "sys.stdout.write('\\rtraining ... (%d in %d sents) use memory %.3f Gb' % (100, 100, get_process_memory()))\n",
    "print('\\rtraining ... (%d in %d sents) use memory %.3f Gb' % (100, 100, get_process_memory()))\n",
    "print('\\rtraining was done. used memory %.3f Gb' % (get_process_memory()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
