{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* merge_variants 결과에 심대한 오류 (간체 텍스트 경우) 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from hanziminer import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# AUTO RELOAD : https://goo.gl/hYKqVS\n",
    "%load_ext autoreload\n",
    "%autoreload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Duplicated Characters were merged\n",
      "# Variants Characters were merged\n",
      "# Comments were removed\n",
      "# Punctuations were removed\n",
      "# Korean, Alphabet, Numbers were removed\n",
      "# Spaces were merged\n",
      "# File _tmp/_dummy_formulas_clean.txt was Created\n"
     ]
    }
   ],
   "source": [
    "text = open(\"_tmp/_dummy_formulas.yml\", 'r', encoding=\"utf-8\").read()\n",
    "corpus = Corpus( text[10000:20000] )\n",
    "corpus.merge_duplications().merge_variants()\n",
    "corpus.remove_comments( comments_header=\"//\" ).remove_punctuation().remove_chrs().merge_spaces()\n",
    "corpus.export(\"_tmp/_dummy_formulas_clean.txt\", plain_text=True )\n",
    "# corpus.export(\"_tmp/_dummy_formulas_docs.txt\", plain_text=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training ... \n",
      "Progress |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.0% Complete\n",
      "# Training was done. Used memory 0.100 Gb\n",
      "# Extracting ...\n",
      "Progress |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.0% Complete\n",
      "# Extrating was done. System memory 0.100 Gb used\n",
      "# 144 of tokens were reported in _tmp/_dummy_formulas_tokens.txt\n"
     ]
    }
   ],
   "source": [
    "te = TokenExtractor( corpus )\n",
    "te.train().extract().report(\"_tmp/_dummy_formulas_tokens.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "sg = SegmenterScore( te.score(), method='cohesion' )\n",
    "input_doc = re.sub( r\"(\\r?\\n){2}\", \"\\n\", corpus.to_text() )\n",
    "output_doc = open(\"_tmp/_dummy_formulas_tokenized.txt\", 'w', encoding=\"utf-8\")\n",
    "for line in input_doc.split(\"\\n\"):\n",
    "    if line.strip() == \"\":\n",
    "        output_doc.write(\"\\n\")\n",
    "    else:\n",
    "        output_doc.write( \" \".join( sg.load( line ).segment().to_list( verbose=False, keyword_only=False ) ) )\n",
    "        output_doc.write(\"\\n\")\n",
    "output_doc.close()\n",
    "print(\"Completed\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['肺郁痰火或陰虛肺熱所致之咳嗽']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sg.load( line ).segment().to_list( verbose=False, keyword_only=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "sg2 = SegmenterGram( gram_size=2 )\n",
    "output_doc = open(\"_tmp/_dummy_formulas_tokenized2.txt\", 'w', encoding=\"utf-8\")\n",
    "for line in input_doc.split(\"\\n\"):\n",
    "    if line.strip() == \"\":\n",
    "        output_doc.write(\"\\n\")\n",
    "    else:\n",
    "        output_doc.write( sg2.load( line ).segment().to_string() )\n",
    "        output_doc.write(\"\\n\")\n",
    "output_doc.close()\n",
    "print(\"Completed\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Co-occurrence Counting ... \n",
      "Progress |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.0% Complete\n",
      "# Co-occurrence Counting was done. System memory 0.091 Gb used\n",
      "# All Tokens were Counted\n",
      "# Co-occurrence Scores Generating ... \n",
      "Progress |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.0% Complete\n",
      "# Co-occurrence Scores were generated. System memory 0.091 Gb used\n"
     ]
    }
   ],
   "source": [
    "txt = open(\"_tmp/_dummy_formulas_tokenized.txt\", 'r', encoding=\"utf-8\").read()\n",
    "cq = COQuantifier().load( txt  ).count().score()\n",
    "cq.export( \"_tmp/_dummy_formulas_당귀.txt\", \"當歸\", cutoff=0.1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Co-occurrence Counting ... \n",
      "Progress |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.0% Complete\n",
      "# Co-occurrence Counting was done. System memory 0.091 Gb used\n",
      "# All Tokens were Counted\n",
      "# Co-occurrence Scores Generating ... \n",
      "Progress |████████████████████████████████████████████████████████████████████████████████████████████████████| 100.0% Complete\n",
      "# Co-occurrence Scores were generated. System memory 0.091 Gb used\n",
      "# Report File was exported \n"
     ]
    }
   ],
   "source": [
    "txt2 = open(\"_tmp/_dummy_formulas_tokenized2.txt\", 'r', encoding=\"utf-8\").read()\n",
    "cq2 = COQuantifier().load( txt2  ).count().score()\n",
    "cq2.export( \"_tmp/_dummy_formulas_당귀2.txt\", \"當歸\", cutoff=0.1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ss = SegmenterDict( [\"a\", \"ab\", \"abc\", \"bc\", \"bcd\", \"cdef\", \"fgh\"] )\n",
    "ss.load(\"abcdefghijkl\").segment().to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sg = Segmenter( te.score(), target_score='cohesion' )\n",
    "# sgl = Segmenter( te.score, target_score='cohesion_l' )\n",
    "# sgr = Segmenter( te.score, target_score='cohesion_r' )\n",
    "sgE = Segmenter( te.score(), target_score='branch_entropy' )\n",
    "\n",
    "\n",
    "docs = [\n",
    "    \"治風證眩暈. 山茱萸肉 一兩, 山藥ㆍ甘菊ㆍ人參ㆍ川芎ㆍ茯神 各五錢. 右爲末, 每二錢, 酒調下. 《本事》\",\n",
    "    \"眞人養生銘曰人欲勞於形百病不能成飮酒勿大醉諸疾自不生食了行百步數以手摩肚寅丑日剪甲頭髮梳百度飽卽立小便飢則坐漩尿行處勿當風居止無小隙常夜濯足臥飽食終無益思慮最傷神喜怒最傷氣每去鼻中毛常習不唾地平明欲起時下床先左脚一日無災殃去邪兼辟惡如能七星步令人長壽樂酸味傷於筋苦味傷於骨甘卽不益肉辛多敗正氣鹹多促人壽不得偏耽嗜春夏少施泄秋冬固陽事獨臥是守眞愼靜最爲貴錢財生有分知足將爲利强知是大患少慾終無累神靜自常安修道宜終始書之屋壁中將以傳君子\",\n",
    "    \"久服明目輕身延年酒浸曝乾蒸之如此九次搗爲末每二錢空心溫酒調服一日二次本草\",\n",
    "    \"治折傷後爲風寒濕所侵手足疼痛生蒼朮破古紙半生半炒骨碎補穿山甲桑灰炒爲珠生草烏各二兩茴香一兩半右將草烏剉如麥大同連皮生薑四兩擂爛淹兩宿焙乾同前藥爲末酒糊和丸梧子大溫酒下五十丸少麻無妨得效\"\n",
    "]\n",
    "\n",
    "for sn in docs:\n",
    "    sg.load( sn ).segment()\n",
    "    #sgl.load( sn ).segment()\n",
    "    #sgr.load( sn ).segment()\n",
    "    sgE.load( sn ).segment()\n",
    "#     print( sgs.token_candis )\n",
    "    #print( sg.target_text )\n",
    "    print( sg.to_string( verbose=True, keyword_only=True ) ) # not working\n",
    "    print( sg.to_string( verbose=True, keyword_only=False ) )\n",
    "    print( sg.to_string( verbose=False, keyword_only=True ) ) \n",
    "    print( sg.to_string( verbose=False, keyword_only=False ) )\n",
    "    \n",
    "    print( sg.to_list( verbose=True, keyword_only=True ) )\n",
    "    print( sg.to_list( verbose=True, keyword_only=False ) ) # not working\n",
    "    print( sg.to_list( verbose=False, keyword_only=True ) )\n",
    "    print( sg.to_list( verbose=False, keyword_only=False ) )\n",
    "    \n",
    "    #print( sgl.show() )\n",
    "    #print( sgr.show() )\n",
    "#     print( sgE.show() )\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SANDBOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.power( 0.7, 1/2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(te.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rst = sorted( te.score.items(), key=lambda it: -1* ( it[1].cohesion_r * it[1].cohesion_l ) )\n",
    "tmp2 = open(\"_tmp/_dummy_corpus_score.txt\", 'w', encoding=\"utf-8\")\n",
    "pp = pprint.PrettyPrinter(indent=4, stream=tmp2)\n",
    "pp.pprint( [ ( r[0], r[1].freq , r[1].cohesion_r, r[1].cohesion_l ) for r in rst ] )\n",
    "\n",
    "print(\"ending\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "te.unigram_counter.get(\"枸\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter(\"huihihu ihuihi\") \n",
    "d = Counter(\"werwwqweqriiiiiiiiiiii\")\n",
    "c.update(d)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "97 / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"c\" in {'a':1, 'b':2}.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict( [('a', 1), ('b', 2)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def entropy( p ):\n",
    "    return -1 * p * math.log2( p )\n",
    "entropy( 100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"123456789\"[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import psutil\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_available_memory():\n",
    "    \"\"\"It returns remained memory as percentage\"\"\"\n",
    "\n",
    "    mem = psutil.virtual_memory()\n",
    "    return 100 * mem.available / (mem.total)\n",
    "\n",
    "def get_process_memory():\n",
    "    \"\"\"It returns the memory usage of current process\"\"\"\n",
    "    \n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / (1024 ** 3)\n",
    "\n",
    "\n",
    "sys.stdout.write('\\rtraining ... (%d in %d sents) use memory %.3f Gb' % (100, 100, get_process_memory()))\n",
    "print('\\rtraining ... (%d in %d sents) use memory %.3f Gb' % (100, 100, get_process_memory()))\n",
    "print('\\rtraining was done. used memory %.3f Gb' % (get_process_memory()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "rg = re.compile(\"{0}\\d+?{0}\".format(\"%\") )\n",
    "# rg = re.compile(\"\\d+?\" )\n",
    "txt = \"朮破古紙半%1%生半炒骨碎%12%補穿山甲桑%8%灰炒爲珠生\"\n",
    "print( re.findall( rg, txt) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x in Counter(['a', 'a', 'b']):\n",
    "    print(x)\n",
    "\n",
    "from itertools import chain, combinations\n",
    "k = Counter(['a', 'a', 'b', 'c'])\n",
    "combinations( k.keys(), 2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = ['\\\\', '|', '/', '-']\n",
    "for i in range(0, 1000):\n",
    "    sys.stdout.write(\"\\r# {}\".format( k[i % 4]) )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "if not re.search( re.compile(\"[ \\t]+\"), \"a1aa1aaa1aaaa1aaaa1\" ): print(\"efef\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k = [0] * 15\n",
    "k[4:6] = [1]*2\n",
    "k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', 'abc', 'de', 'fgh', 'ijkl']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ss = SegmenterDict( [\"a\", \"ab\", \"abc\", \"bc\", \"bcd\", \"cdef\", \"fgh\"] )\n",
    "ss.load(\"abcdefghijkl\").segment().to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'wer'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r\"wer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
