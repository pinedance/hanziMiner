{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ncorpus_path = \"./CORPUS/DYBG.txt\"\\n# corpus_path = \"./CORPUS/DYBG_tn.txt\"\\n\\ncorpus = DoublespaceLineCorpus(corpus_path, iter_sent=True)\\nprint(len(corpus)) # 223,357\\n\\nword_extractor = WordExtractor(min_count=5)\\nword_extractor.train( corpus )\\nword_scores = word_extractor.extract()\\n'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "from collections import namedtuple\n",
    "from collections import Counter\n",
    "import math\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# KMcorpus\n",
    "\n",
    "class KMcorpus:\n",
    "    \n",
    "    PC = \"[,\\.!\\?！＇，ㆍ．／：；？｀、。·‥…¨〃∼´～˝\\(\\)\\{\\}\\[\\]（）［］｛｝‘’“”〔〕〈〉《》「」『』【】]\"\n",
    "    \n",
    "    def __init__( self, text, comments_header=\"#\", doc_sep=\"\\r?\\n\\r?\\n\" ):\n",
    "        self.text = text\n",
    "        self.doc_sep = doc_sep\n",
    "        self.comments_header = comments_header\n",
    "        self.docs = []\n",
    "        print(self.text[0:100])\n",
    "        \n",
    "    def remove_comments( self ):\n",
    "        pattern = \"{}.*?$\".format( self.comments_header )\n",
    "        regex = re.compile( pattern, re.MULTILINE|re.DOTALL )\n",
    "        self.text = re.sub( regex, \"\", self.text ).strip()\n",
    "        return self\n",
    "        \n",
    "    def remove_punctuation( self ):  \n",
    "        regex_PC = re.compile( self.PC )\n",
    "        self.text = re.sub( regex_PC , \"\", self.text ).strip()\n",
    "        return self\n",
    "    \n",
    "    def remove_chrs( self, chr_types=[\"Korean\", \"Alphabet\", \"Numbers\"] ):\n",
    "        if \"Korean\" in chr_types:\n",
    "            self.text = re.sub( re.compile(\"[가-힣]\"), \"\", self.text )\n",
    "        if \"Alphabet\" in chr_types:\n",
    "            self.text = re.sub( re.compile(\"[a-zA-Z]\"), \"\", self.text )\n",
    "        if \"Numbers\" in chr_types:\n",
    "            self.text = re.sub( re.compile(\"[\\d]+?\"), \"\", self.text )\n",
    "        self.text = self.text.strip()\n",
    "        return self\n",
    "    \n",
    "    def merge_spaces( self ):\n",
    "        self.text = re.sub( re.compile(\"[ \\t]+?\"), \" \", self.text )\n",
    "        self.text = re.sub( re.compile(\"^[ \\t]+?\", re.MULTILINE|re.DOTALL), \"\", self.text ).strip()\n",
    "        return self\n",
    "        \n",
    "    def text2docs(self):\n",
    "        docs = re.split( re.compile( self.doc_sep ), self.text )\n",
    "        self.docs = [ doc.strip().split() for doc in docs ]\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def merge_dupCodepages():\n",
    "        \"\"\"\"\"\"\n",
    "    \n",
    "    def merge_variants():\n",
    "        \"\"\"\"\"\"\n",
    "    \n",
    "\n",
    "# Segment\n",
    "\n",
    "class TokenExtractor:\n",
    "    \n",
    "    def __init__( self, corpus, min_freq = 5 ):\n",
    "        self.corpus = corpus\n",
    "        self.min_freq = min_freq\n",
    "        self.token_counter = Counter()\n",
    "        self.unigram_counter = Counter( self.corpus.text )\n",
    "        self.score = {}\n",
    "        \n",
    "    def train( self, method=\"allgram\", min_window=2, max_window=8, ngram_size=2 ):\n",
    "        self.min_window = min_window\n",
    "        if method == \"allgram\" :\n",
    "            for doc in self.corpus.docs:\n",
    "                for phrase in doc:\n",
    "                    paticles = self.allgram( phrase, min_window, max_window )\n",
    "                    self.token_counter.update( Counter(paticles) )\n",
    "        else:\n",
    "            for doc in self.corpus.docs:\n",
    "                for phrase in doc:\n",
    "                    paticles = self.ngram( phrase, ngram_size )\n",
    "                    self.token_counter.update( Counter(paticles) )\n",
    "            \n",
    "        self.token_counter = Counter( {x : self.token_counter[x] for x in self.token_counter if self.token_counter[x] >= self.min_freq } )\n",
    "        return self\n",
    "        \n",
    "    def ngram( self, text, n):\n",
    "        return [ text[i:i+n] for i in range( 0, len(text) - n + 1 )  ]\n",
    "\n",
    "    def allgram( self, text, min_window=2, max_window=8 ):\n",
    "        len_txt = len(text)\n",
    "        mx_wd = len_txt if ( len_txt < max_window ) else max_window\n",
    "        rst = []\n",
    "        for i in range(min_window, mx_wd + 1):\n",
    "            rst += ngram(text, i)\n",
    "        return rst\n",
    "\n",
    "    def cohesion_score( self, word ):\n",
    "        word_len = len( word )\n",
    "        first_chr_freq = self.unigram_counter[ word[0] ]\n",
    "        whole_word_freq = self.token_counter[ word ]\n",
    "        \n",
    "        if (not word) or ( word_len < self.min_window ):\n",
    "            return (0, 0)\n",
    "        cohesion = 0 if whole_word_freq == 0 else np.power( ( whole_word_freq / first_chr_freq ), (1 / (word_len - 1)) )\n",
    "        return cohesion  \n",
    "\n",
    "    def get_score( self ):\n",
    "        \n",
    "        tmp = namedtuple('Score', ['freq', 'cohension_score'])\n",
    "        \n",
    "        for x in self.token_counter:\n",
    "            \n",
    "            tmp.freq = self.token_counter[x]\n",
    "            tmp.cohension_score = self.cohesion_score( x )\n",
    "            self.score[x] = tmp\n",
    "        \n",
    "        return self\n",
    "            \n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "corpus_path = \"./CORPUS/DYBG.txt\"\n",
    "# corpus_path = \"./CORPUS/DYBG_tn.txt\"\n",
    "\n",
    "corpus = DoublespaceLineCorpus(corpus_path, iter_sent=True)\n",
    "print(len(corpus)) # 223,357\n",
    "\n",
    "word_extractor = WordExtractor(min_count=5)\n",
    "word_extractor.train( corpus )\n",
    "word_scores = word_extractor.extract()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "// 이 파일은 한국한의학연구원에서 동의보감 원문을 디지타이즈하여 배포한 것입니다. \n",
      "// File Info: { encoding: 'UTF-8', end_of_line: 'LF'\n",
      "ending\n"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "\n",
    "text = open(\"_dummy_corpus.txt\", 'r', encoding=\"utf-8\").read()\n",
    "corpus = KMcorpus(text, comments_header=\"//\")\n",
    "corpus.remove_comments().remove_punctuation().remove_chrs().merge_spaces().text2docs()\n",
    "\n",
    "tmp = open(\"_dummy_corpus_clean.txt\", 'w', encoding=\"utf-8\")\n",
    "pp = pprint.PrettyPrinter(indent=4, stream=tmp)\n",
    "pp.pprint( corpus.docs )\n",
    "\n",
    "\n",
    "te = TokenExtractor( corpus )\n",
    "te.train().get_score()\n",
    "\n",
    "tmp2 = open(\"_dummy_corpus_score.txt\", 'w', encoding=\"utf-8\")\n",
    "pp = pprint.PrettyPrinter(indent=4, stream=tmp2)\n",
    "pp.pprint( [ (x, te.score[x].freq, te.score[x].cohension_score) for x in te.score ] )\n",
    "\n",
    "print(\"ending\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntext = \"1234567890\"\\ntext2 = \"1234\"\\ntext3 = \"12\"\\n\\nprint( ngram(text, 5) )\\nprint( allgram(text))\\nprint( allgram(text2))\\nprint( allgram(text2, 1))\\nprint( allgram(text3, min_windows=3 ))\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "text = \"1234567890\"\n",
    "text2 = \"1234\"\n",
    "text3 = \"12\"\n",
    "\n",
    "print( ngram(text, 5) )\n",
    "print( allgram(text))\n",
    "print( allgram(text2))\n",
    "print( allgram(text2, 1))\n",
    "print( allgram(text3, min_windows=3 ))\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'self.text'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = \"{}[^\\r\\n]*$\".format( \"//\" )\n",
    "regex = re.compile( pattern )\n",
    "re.sub( regex, \"\", \"self.text//cooment\\n//jiojoijoijio\\nhiuojiojio\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'i': 17, 'h': 5, 'w': 4, 'u': 3, 'e': 2, 'r': 2, 'q': 2, ' ': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "c = Counter(\"huihihu ihuihi\") \n",
    "d = Counter(\"werwwqweqriiiiiiiiiiii\")\n",
    "c.update(d)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
