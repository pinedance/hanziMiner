{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import psutil\n",
    "import sys\n",
    "\n",
    "class Tools:\n",
    "    # Frome here : https://github.com/lovit/soynlp/tree/master/soynlp/utils\n",
    "    @staticmethod\n",
    "    def get_available_memory():\n",
    "        \"\"\"It returns remained memory as percentage\"\"\"\n",
    "\n",
    "        mem = psutil.virtual_memory()\n",
    "        return 100 * mem.available / (mem.total)\n",
    "\n",
    "    def get_process_memory():\n",
    "        \"\"\"It returns the memory usage of current process\"\"\"\n",
    "\n",
    "        process = psutil.Process(os.getpid())\n",
    "        return process.memory_info().rss / (1024 ** 3)\n",
    "\n",
    "    def progress_symbol():\n",
    "        return ['\\\\', '|', '/', '―']\n",
    "    \n",
    "    # Print iterations progress\n",
    "    def print_progress(iteration, total, prefix='Progress', suffix='Complete', decimals=1, bar_length=100):\n",
    "        \"\"\"\n",
    "        Call in a loop to create terminal progress bar\n",
    "        @params:\n",
    "            iteration   - Required  : current iteration (Int)\n",
    "            total       - Required  : total iterations (Int)\n",
    "            prefix      - Optional  : prefix string (Str)\n",
    "            suffix      - Optional  : suffix string (Str)\n",
    "            decimals    - Optional  : positive number of decimals in percent complete (Int)\n",
    "            bar_length  - Optional  : character length of bar (Int)\n",
    "        \"\"\"\n",
    "        str_format = \"{0:.\" + str(decimals) + \"f}\"\n",
    "        percents = str_format.format(100 * (iteration / float(total)))\n",
    "        filled_length = int(round(bar_length * iteration / float(total)))\n",
    "        bar = '█' * filled_length + '-' * (bar_length - filled_length)\n",
    "\n",
    "        sys.stdout.write('\\r%s |%s| %s%s %s' % (prefix, bar, percents, '%', suffix)),\n",
    "\n",
    "        if iteration == total:\n",
    "            sys.stdout.write('\\n')\n",
    "        sys.stdout.flush()\n",
    "        # ref : https://gist.github.com/aubricus/f91fb55dc6ba5557fbab06119420dd6a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# KMcorpus\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import namedtuple\n",
    "from collections import Counter\n",
    "import math\n",
    "import re\n",
    "import yaml\n",
    "\n",
    "class KMcorpus:\n",
    "    \n",
    "    def __init__( self, text, doc_sep=\"(\\r?\\n){2,}\" ):\n",
    "        self.text = text\n",
    "        self.doc_sep = doc_sep\n",
    "        self.docs = []\n",
    "        \n",
    "    def remove_comments( self, comments_header=\"#\" ):\n",
    "        _comments_pattern = \"{}.*?$\".format( comments_header )\n",
    "        self.comments_pattern = re.compile( _comments_pattern, re.MULTILINE|re.DOTALL )\n",
    "        self.text = re.sub( self.comments_pattern, \" \", self.text ).strip()\n",
    "        print(\"# Comments were removed\")\n",
    "        return self\n",
    "        \n",
    "    def remove_punctuation( self, punctuations=\"[,\\.!\\?！\\:;＇，ㆍ．／：；？｀、。·‥…¨〃∼´～˝\\-\\\\\\(\\)\\{\\}\\[\\]（）［］｛｝‘’“”〔〕〈〉《》「」『』【】%\\$]\" ): \n",
    "        self.punctuations_pattern = re.compile( punctuations )\n",
    "        self.text = re.sub( self.punctuations_pattern , \" \", self.text ).strip()\n",
    "        print(\"# Punctuations were removed\")\n",
    "        return self\n",
    "    \n",
    "    def remove_chrs( self, chr_types=[\"Korean\", \"Alphabet\", \"Numbers\"] ):\n",
    "        if \"Korean\" in chr_types:\n",
    "            self.text = re.sub( re.compile(\"[가-힣]\"), \" \", self.text )\n",
    "        if \"Alphabet\" in chr_types:\n",
    "            self.text = re.sub( re.compile(\"[a-zA-Z]\"), \" \", self.text )\n",
    "        if \"Numbers\" in chr_types:\n",
    "            self.text = re.sub( re.compile(\"[\\d]+?\"), \" \", self.text )\n",
    "        self.text = self.text.strip()\n",
    "        print(\"# {} were removed\".format( \", \".join( chr_types  ) ) )\n",
    "        return self\n",
    "    \n",
    "    def merge_spaces( self ):\n",
    "        self.text = re.sub( re.compile(\"[ \\t]+\"), \" \", self.text )\n",
    "        self.text = re.sub( re.compile(\"^[ \\t]+\", re.MULTILINE|re.DOTALL), \"\", self.text ).strip()\n",
    "        print(\"# Spaces were merged\")\n",
    "        return self\n",
    "        \n",
    "    def text2docs(self):\n",
    "        docs = re.split( re.compile( self.doc_sep ), self.text )\n",
    "        self.docs = [ doc.strip().split() for doc in docs ]\n",
    "        print(\"# Text was converted to List Data\")\n",
    "        return self\n",
    "    \n",
    "    def merge_duplications(self, dict_path=\"dicts/duplications.dic\" ):\n",
    "        print(\"# Duplicated Characters were merged\")\n",
    "        self.text = self.__class__.merge_chrs( self.text, dict_path ) \n",
    "        return self\n",
    "    \n",
    "    def merge_variants(self, dict_path=\"dicts/variants.dic\" ):\n",
    "        print(\"# Variants Characters were merged\")\n",
    "        self.text = self.__class__.merge_chrs( self.text, dict_path ) \n",
    "        return self\n",
    "    \n",
    "    def export(self, output_filename, plain_text=True ):\n",
    "        stream = open(output_filename, 'w', encoding=\"utf-8\")\n",
    "        if plain_text:\n",
    "            stream.write( self.text )\n",
    "        else:\n",
    "            yaml.dump( self.docs, stream, default_flow_style=False,  allow_unicode=True )\n",
    "        stream.close()\n",
    "        print( \"# File {} was Created\".format( output_filename ) )\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    def merge_chrs(text, dict_path ):\n",
    "        dic = open(dict_path, 'r', encoding='utf-8').readlines()\n",
    "        text_ = text + \"\"\n",
    "        for pair in dic:\n",
    "            a, b = pair.split(\"\\t\")\n",
    "            text_ = text_.replace(a, b)\n",
    "        return text_\n",
    "    \n",
    "    def ngram( text, n):\n",
    "        return [ text[i:i+n] for i in range( 0, len(text) - n + 1 )  ]\n",
    "\n",
    "    def allgram( text, min_window=2, max_window=8 ):\n",
    "        len_txt = len(text)\n",
    "        mx_wd = len_txt if ( len_txt < max_window ) else max_window\n",
    "        rst = []\n",
    "        for i in range(min_window, mx_wd + 1):\n",
    "            rst += KMcorpus.ngram(text, i)\n",
    "        return rst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Segment\n",
    "\n",
    "from collections import defaultdict\n",
    "from collections import namedtuple\n",
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "\n",
    "class TokenExtractor:\n",
    "    \n",
    "    def __init__( self, corpus ):\n",
    "        self.corpus = corpus\n",
    "        self.token_counter = Counter()\n",
    "        self.unigram_counter = Counter( self.corpus.text )\n",
    "        self.bigram_counter = Counter()\n",
    "        \n",
    "    def _cohesion_score( self, word ):\n",
    "        word_len = len( word )\n",
    "        if (not word) or ( word_len < self.min_window ):\n",
    "            return 0\n",
    "        \n",
    "        first_chr_freq = self.unigram_counter[ word[0] ]\n",
    "        last_chr_freq = self.unigram_counter[ word[-1] ]\n",
    "        whole_word_freq = self.token_counter[ word ]\n",
    "        \n",
    "        cohesion_l = 0 if whole_word_freq == 0 else math.pow( ( whole_word_freq / first_chr_freq ), (1 / (word_len - 1)) )\n",
    "        cohesion_r = 0 if whole_word_freq == 0 else math.pow( ( whole_word_freq / last_chr_freq ), (1 / (word_len - 1)) )\n",
    "        cohesion = math.sqrt(cohesion_l * cohesion_r)\n",
    "        return ( cohesion_l, cohesion_r, cohesion , (cohesion_l + cohesion_r)/2 )\n",
    "\n",
    "    def _branch_entropy_score( self, word ):\n",
    "        word_len = len( word )\n",
    "        whole_word_freq = self.token_counter[ word ]\n",
    "        token_l, token_r = word[:-1], word[1:]        \n",
    "        branch_entropy_l = self.__class__.entropy( whole_word_freq / self.token_counter[token_l] ) if ( token_l in self.token_counter ) and (self.token_counter[token_l] != 0 ) else 0\n",
    "        branch_entropy_r = self.__class__.entropy( whole_word_freq / self.token_counter[token_r] ) if ( token_r in self.token_counter ) and (self.token_counter[token_r] != 0 ) else 0\n",
    "        \n",
    "        # debuging ###\n",
    "        if not( ( token_l in self.token_counter ) and (self.token_counter[token_l] != 0 ) ):\n",
    "            print(\"token_l\", word, token_l, self.token_counter[token_l] )\n",
    "            \n",
    "        if not ( ( token_r in self.token_counter ) and (self.token_counter[token_r] != 0 ) ):\n",
    "            print(\"token_r\", word, token_r, self.token_counter[token_r] )\n",
    "        ### \n",
    "        \n",
    "        return ( ( token_l, branch_entropy_l ), ( token_r, branch_entropy_r ) )\n",
    "\n",
    "   # return self\n",
    "        \n",
    "    def train( self, min_freq = 5, min_window=2, max_window=8  ):\n",
    "        self.min_freq = min_freq\n",
    "        self.max_window = max_window\n",
    "        if min_window < 2:\n",
    "            self.min_window = 2\n",
    "            print(\"!!! Min_window must be greater than 2. Automatically set 2\")\n",
    "        else:\n",
    "            self.min_window = min_window\n",
    "        \n",
    "        corpus_size = len(self.corpus.docs)\n",
    "        \n",
    "        print(\"# Training ... \")\n",
    "        for i, doc in enumerate(self.corpus.docs):\n",
    "            sys.stdout.flush()\n",
    "            Tools.print_progress(i, corpus_size, prefix='Progress', suffix='Complete')\n",
    "            for phrase in doc:\n",
    "                particles = KMcorpus.allgram( phrase, min_window- 1 , max_window + 1 ) # branch entropy를 구히기 위해 window 범위를 1씩 늘림\n",
    "                self.token_counter.update( Counter( particles ) )\n",
    "\n",
    "                bigrams = KMcorpus.ngram( phrase, n=2 )\n",
    "                self.bigram_counter.update( Counter( bigrams ) )\n",
    "\n",
    "        # Branch Entropy\n",
    "        self._total_branch_entropy_score()\n",
    "        print( \"# Training was done. Used memory {:.3f} Gb\".format( Tools.get_process_memory() ) )\n",
    "        return self\n",
    "        \n",
    "    def _total_branch_entropy_score( self ):\n",
    "        branch_entropy_l = defaultdict(lambda: 0)\n",
    "        branch_entropy_r = defaultdict(lambda: 0)\n",
    "        for (w, f) in self.token_counter.items():\n",
    "            if ( len(w) < self.min_window ): continue\n",
    "            be_l, be_r = self._branch_entropy_score( w )\n",
    "            branch_entropy_l[ be_l[0] ] += be_l[1]\n",
    "            branch_entropy_r[ be_r[0] ] += be_r[1]\n",
    "        self.total_branch_entropy_l = branch_entropy_l\n",
    "        self.total_branch_entropy_r = branch_entropy_r\n",
    "        return self\n",
    "    \n",
    "    def extract( self ):\n",
    "        self._score = defaultdict()\n",
    "        self._score_header = ['freq', 'cohesion_l', 'cohesion_r', 'cohesion', 'cohesion_s', 'branch_entropy_l', 'branch_entropy_r', 'branch_entropy' ]\n",
    "        _score = namedtuple('Score', self._score_header )\n",
    "        i = 0\n",
    "        total_len = len( self.token_counter )\n",
    "        print(\"\\r# Extracting ...\" )\n",
    "        for (w, f) in self.token_counter.items():\n",
    "            if len(w) > self.max_window: continue\n",
    "            if len(w) < self.min_window: continue\n",
    "            if f < self.min_freq: continue\n",
    "\n",
    "            _freq = f\n",
    "            # Cohesion Score\n",
    "            _cohesion_l, _cohesion_r, _cohesion, _cohesion_s  = self._cohesion_score( w )\n",
    "            # Branch Entropy Score\n",
    "            _branch_entropy_l = self.total_branch_entropy_l[w]\n",
    "            _branch_entropy_r = self.total_branch_entropy_r[w]\n",
    "            _branch_entropy = ( _branch_entropy_l + _branch_entropy_r ) / 2\n",
    "            self._score[ w ] = _score( _freq, _cohesion_l, _cohesion_r, _cohesion, _cohesion_s, _branch_entropy_l, _branch_entropy_r, _branch_entropy )\n",
    "            \n",
    "            # Report progress\n",
    "            Tools.print_progress(i, total_len, prefix='Progress', suffix='Complete')\n",
    "            i += 1\n",
    "\n",
    "        print(\"# Extrating was done. System memory {:.3f} Gb used\".format( Tools.get_process_memory()) )\n",
    "        return self\n",
    "    \n",
    "    # get score\n",
    "    def score(self):\n",
    "        return self._score\n",
    "    \n",
    "    def report(self, output_filename, sep=\"\\t\", order=\"cohesion\"):\n",
    "        handler = open(output_filename, 'w', encoding=\"utf-8\")\n",
    "        header = \"token\" + sep + sep.join( self._score_header ) + \"\\n\"\n",
    "        handler.write(header)\n",
    "\n",
    "        _score_list = self.score().items()\n",
    "        score_list = sorted( _score_list, key=lambda x: getattr( x[1], order ), reverse=True )\n",
    "        for word, score in score_list:\n",
    "            handler.write( word + sep + sep.join( [ \"{:01.3f}\".format( getattr( score, s ) ) for s in self._score_header ]) + \"\\n\" )\n",
    "        handler.close()\n",
    "        print(\"# {:d} of tokens were reported in {}\".format( len( score_list) , output_filename  ) )\n",
    "                          \n",
    "    @staticmethod\n",
    "    def entropy( p ):\n",
    "        return -1 * p * math.log2( p )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import namedtuple\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "class Segmenter:\n",
    "    \n",
    "    def __init__( self, token_with_score, target_score=\"cohesion\", score_cutoff=0):\n",
    "        self.score_list = [ ( tk, getattr( sc, target_score ) ) for tk, sc in token_with_score.items() if getattr( sc, target_score ) >= score_cutoff  ]\n",
    "        self.score = dict( self.score_list )\n",
    "        self.tokens = self.score.keys()\n",
    "        self.target_text = \"\"\n",
    "        \n",
    "    def load( self, text, min_window=2, max_window=8 ):\n",
    "        _token_candis = set( KMcorpus.allgram(text, min_window, max_window) )\n",
    "        token_candis_with_score = [ ( it, self.score[it] ) for it in _token_candis if it in self.tokens ]\n",
    "        self.token_candis = sorted( token_candis_with_score, key=lambda x: (-x[1], -len(x[0] )  ) )\n",
    "        self.target_text = text\n",
    "        return self\n",
    "    \n",
    "    def segment( self, segment_marker=\"%\" ):\n",
    "        target_text = self.target_text + \"\"\n",
    "        self.segment_marker = segment_marker\n",
    "        for i, candi in enumerate( self.token_candis ):\n",
    "            marker = \"{0}{0}{1}{0}{0}\".format( self.segment_marker, i )\n",
    "            target_text = marker.join( target_text.split( candi[0] ) )\n",
    "        \n",
    "        self.text_segment_marked = target_text\n",
    "        return self\n",
    "    \n",
    "    def to_string( self, verbose=False, keyword_only=False, sep=\" \" ):\n",
    "        target_text = self.text_segment_marked + \"\"\n",
    "        if keyword_only:\n",
    "            self.text_segmented = sep.join( self.to_list(verbose=False, keyword_only=True) )\n",
    "        else:\n",
    "            for i, candi in enumerate( self.token_candis ):\n",
    "                marker = \"{0}{0}{1}{0}{0}\".format( self.segment_marker, i )\n",
    "                seg = \"【{0}/{1:01.3f}】\".format( candi[0], candi[1] ) if verbose else \"【{}】\".format( candi[0] )\n",
    "                target_text = target_text.replace(marker, seg )\n",
    "            self.text_segmented = target_text    \n",
    "        return self.text_segmented\n",
    "\n",
    "    def to_list( self, verbose=False, keyword_only=False ):\n",
    "        target_text = self.text_segment_marked + \"\"\n",
    "        \n",
    "        if keyword_only:\n",
    "            rg = re.compile( \"\\{0}\\{0}\\d+?\\{0}\\{0}\".format( self.segment_marker ) )\n",
    "            _segment_list = re.findall( rg, target_text )\n",
    "            segment_list = [ self.token_candis[ int(it[2:-2]) ] for it in _segment_list ] if verbose else [ self.token_candis[ int(it[2:-2]) ][0] for it in _segment_list ]\n",
    "        else:\n",
    "            segment_list= re.split(r\"[【】]\", self.to_string( verbose=False, keyword_only=False ) )\n",
    "        self.list_segmented = list( filter( None, segment_list ) )\n",
    "        return self.list_segmented\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from collections import namedtuple\n",
    "from collections import Counter\n",
    "import re\n",
    "from itertools import chain, combinations\n",
    "\n",
    "class COQuantifier:  # co-occurrence \n",
    "    \n",
    "    score_header = ['total_freq', 'observed_cooccurrence', 'expected_cooccurrence', 't_score']\n",
    "    \n",
    "    def __init__( self ):\n",
    "        \"\"\"\"\"\"\n",
    "\n",
    "    def load( self, text_segmented, text_pairing=False, doc_sep=\"(\\r?\\n){2,}\", token_sep=\"[\\s]+\"  ):\n",
    "        self.text = text_segmented\n",
    "        self.text_pairing = text_pairing\n",
    "        self.doc_sep = doc_sep\n",
    "        self.docs = list( map( lambda x: x.strip(), re.split( re.compile( self.doc_sep ), self.text ) ) )\n",
    "        self.token_sep = token_sep\n",
    "        return self\n",
    "    \n",
    "    def count( self ):\n",
    "        self._count_freq()._count_cooccurrence()\n",
    "        print(\"# All Tokens were Counted\")\n",
    "        return self\n",
    "    \n",
    "    def _count_freq( self ):\n",
    "        \"\"\" 전체 단어 빈도 조사 \"\"\"\n",
    "        self.tokens = self.__class__.tokenize( self.text, self.token_sep )\n",
    "        self.token_size = len( self.tokens )\n",
    "        self.token_freq = Counter( self.tokens )\n",
    "        return self\n",
    "    \n",
    "    def _count_cooccurrence( self, allow_duplicate_counts=True ): #doc 안에 같이 나오면 같이 등장하는 것으로.\n",
    "        \"\"\" 문단 별 공기어 빈도 누적 (한 문단 안에 중복해 나와도 거듭 카운트) \"\"\"\n",
    "        _cooccurrence = defaultdict(lambda: 0)\n",
    "        doc_size = len( self.docs )\n",
    "        print(\"\\r# Co-occurrence Counting ... \")\n",
    "        for i, doc in enumerate(self.docs):        \n",
    "            _lines = re.split(r\"\\r?\\n\", doc)\n",
    "            _count = Counter()\n",
    "            for line in _lines:\n",
    "                _count.update( Counter( self.__class__.tokenize( doc, self.token_sep ) ) )\n",
    "            _keys = _count.keys()\n",
    "            for pair in combinations( _keys, 2 ) :\n",
    "                _cooccurrence[ pair ] += _count[ pair[1] ]\n",
    "            Tools.print_progress(i, doc_size, prefix='Progress', suffix='Complete')\n",
    "        self.cooccurrence = _cooccurrence\n",
    "        sys.stdout.flush()\n",
    "        print(\"\\n# Co-occurrence Counting was done. System memory {:.3f} Gb used\".format( Tools.get_process_memory()) )\n",
    "        return self\n",
    "    \n",
    "    def scoring( self ):\n",
    "        # t-score\n",
    "        self.scores = defaultdict( lambda: 0)\n",
    "        # template\n",
    "        _tpl = namedtuple(\"Scores\", self.score_header )\n",
    "        total_len = len( self.cooccurrence )\n",
    "        print(\"\\r# Co-occurrence Scores Generating ... \") \n",
    "        i = 0\n",
    "        for a, b in self.cooccurrence:\n",
    "            _freq_a, _freq_b = self.token_freq[a], self.token_freq[b]\n",
    "            _expected = ( _freq_a * _freq_b ) / self.token_size\n",
    "            _observed = self.cooccurrence[ (a,b) ]\n",
    "            _t_score = self.__class__.get_Tscore( _observed, _expected )\n",
    "            self.scores[ (a,b) ] = _tpl( ( _freq_a, _freq_b ),  _observed, _expected,  _t_score ) \n",
    "            Tools.print_progress(i, total_len, prefix='Progress', suffix='Complete')\n",
    "            i += 1\n",
    "        sys.stdout.flush()\n",
    "        print(\"# Co-occurrence Scores were generated. System memory {:.3f} Gb used\".format( Tools.get_process_memory() ) )\n",
    "        return self\n",
    "    \n",
    "    def export( self, file_name, target_token, cutoff=1.5  ):\n",
    "        stream = open(file_name, 'w', encoding=\"utf-8\")\n",
    "        stream.write( \"\\t\".join( [ \"tokens\" ] + self.score_header ) + \"\\n\" )\n",
    "        _rst = self.report( target_token, cutoff )\n",
    "        for token, score in _rst:\n",
    "            _tmp = list( score )\n",
    "            _total_freq = [ str( _tmp[0][1] ) ]\n",
    "            _etc = list( map( lambda x: \"{:.3f}\".format(x), _tmp[1:] ) )\n",
    "            stream.write( \"\\t\".join(  [ token ] + _total_freq + _etc )  + \"\\n\" )\n",
    "        print(\"# Report File was exported \")\n",
    "            \n",
    "            \n",
    "    def report( self, target_token, cutoff=1.5 ):\n",
    "        _rst = [ ( b, self.scores[(a,b)] ) for (a, b) in self.scores if a == target_token ]\n",
    "        _rst_gt = [ (b, sc) for b, sc in _rst if sc.t_score >= cutoff ]\n",
    "        return sorted( _rst_gt, key=lambda x: x[1].t_score, reverse=True )\n",
    "\n",
    "        \n",
    "    @staticmethod\n",
    "    def tokenize( line, sep ):\n",
    "        _tokens = list( map( lambda x: x.strip(), re.split( re.compile( sep ), line ) ) )\n",
    "        tokens = list( filter( None, _tokens ) )    # remove empty string\n",
    "        return tokens\n",
    "        \n",
    "    def get_Tscore( observed , expected ): \n",
    "        if observed == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return float( observed - expected ) / math.sqrt( observed )  \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Duplicated Characters were merged\n",
      "# Comments were removed\n",
      "# Punctuations were removed\n",
      "# Korean, Alphabet, Numbers were removed\n",
      "# Spaces were merged\n",
      "# Text was converted to List Data\n",
      "# File _tmp/_dummy_formulas_clean.txt was Created\n"
     ]
    }
   ],
   "source": [
    "text = open(\"_tmp/_dummy_formulas.yml\", 'r', encoding=\"utf-8\").read()\n",
    "corpus = KMcorpus(text[0:10000])\n",
    "corpus.merge_duplications().remove_comments( comments_header=\"//\" ).remove_punctuation().remove_chrs().merge_spaces().text2docs()\n",
    "corpus.export(\"_tmp/_dummy_formulas_clean.txt\", plain_text=True )\n",
    "# corpus.export(\"_tmp/_dummy_formulas_docs.txt\", plain_text=False )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Training ... \n",
      "Progress |███████████████████████████████████████████████████████████████████████████████████████████████████-| 99.4% Complete# Training was done. Used memory 0.493 Gb\n",
      "# Extracting ...\n",
      "Progress |███-------------------------------------------------------------------------------------------------| 3.1% Complete# Extrating was done. System memory 0.493 Gb used\n",
      "# 137 of tokens were reported in _tmp/_dummy_formulas_tokens.txt\n"
     ]
    }
   ],
   "source": [
    "te = TokenExtractor( corpus )\n",
    "te.train().extract().report(\"_tmp/_dummy_formulas_tokens.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed\n"
     ]
    }
   ],
   "source": [
    "sg = Segmenter( te.score(), target_score='cohesion' )\n",
    "input_doc = open(\"_tmp/_dummy_formulas_clean.txt\", 'r', encoding=\"utf-8\")\n",
    "output_doc = open(\"_tmp/_dummy_formulas_tokenized.txt\", 'w', encoding=\"utf-8\")\n",
    "for line in input_doc.readlines():\n",
    "    if line.strip() == \"\":\n",
    "        output_doc.write(\"\\n\")\n",
    "    else:\n",
    "        output_doc.write( sg.load( line ).segment().to_string( verbose=False, keyword_only=False ) )\n",
    "        output_doc.write(\"\\n\")\n",
    "print(\"Completed\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Co-occurrence Counting ... \n",
      "Progress |----------------------------------------------------------------------------------------------------| 0.0% Complete\n",
      "# Co-occurrence Counting was done. System memory 0.493 Gb used\n",
      "# All Tokens were Counted\n",
      "# Co-occurrence Scores Generating ... \n",
      "# Co-occurrence Scores were generated. System memory 0.493 Gb used\n"
     ]
    }
   ],
   "source": [
    "txt = open(\"_tmp/_dummy_formulas_tokenized.txt\", 'r', encoding=\"utf-8\").read()\n",
    "cq = COQuantifier().load( txt[0:10000], text_pairing=False ).count().scoring()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Report File was exported \n"
     ]
    }
   ],
   "source": [
    "cq.export( \"_tmp/_dummy_formulas_대황.txt\", \"大黄\", cutoff=0.1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sg = Segmenter( te.score(), target_score='cohesion' )\n",
    "# sgl = Segmenter( te.score, target_score='cohesion_l' )\n",
    "# sgr = Segmenter( te.score, target_score='cohesion_r' )\n",
    "sgE = Segmenter( te.score(), target_score='branch_entropy' )\n",
    "\n",
    "\n",
    "docs = [\n",
    "    \"治風證眩暈. 山茱萸肉 一兩, 山藥ㆍ甘菊ㆍ人參ㆍ川芎ㆍ茯神 各五錢. 右爲末, 每二錢, 酒調下. 《本事》\",\n",
    "    \"眞人養生銘曰人欲勞於形百病不能成飮酒勿大醉諸疾自不生食了行百步數以手摩肚寅丑日剪甲頭髮梳百度飽卽立小便飢則坐漩尿行處勿當風居止無小隙常夜濯足臥飽食終無益思慮最傷神喜怒最傷氣每去鼻中毛常習不唾地平明欲起時下床先左脚一日無災殃去邪兼辟惡如能七星步令人長壽樂酸味傷於筋苦味傷於骨甘卽不益肉辛多敗正氣鹹多促人壽不得偏耽嗜春夏少施泄秋冬固陽事獨臥是守眞愼靜最爲貴錢財生有分知足將爲利强知是大患少慾終無累神靜自常安修道宜終始書之屋壁中將以傳君子\",\n",
    "    \"久服明目輕身延年酒浸曝乾蒸之如此九次搗爲末每二錢空心溫酒調服一日二次本草\",\n",
    "    \"治折傷後爲風寒濕所侵手足疼痛生蒼朮破古紙半生半炒骨碎補穿山甲桑灰炒爲珠生草烏各二兩茴香一兩半右將草烏剉如麥大同連皮生薑四兩擂爛淹兩宿焙乾同前藥爲末酒糊和丸梧子大溫酒下五十丸少麻無妨得效\"\n",
    "]\n",
    "\n",
    "for sn in docs:\n",
    "    sg.load( sn ).segment()\n",
    "    #sgl.load( sn ).segment()\n",
    "    #sgr.load( sn ).segment()\n",
    "    sgE.load( sn ).segment()\n",
    "#     print( sgs.token_candis )\n",
    "    #print( sg.target_text )\n",
    "    print( sg.to_string( verbose=True, keyword_only=True ) ) # not working\n",
    "    print( sg.to_string( verbose=True, keyword_only=False ) )\n",
    "    print( sg.to_string( verbose=False, keyword_only=True ) ) \n",
    "    print( sg.to_string( verbose=False, keyword_only=False ) )\n",
    "    \n",
    "    print( sg.to_list( verbose=True, keyword_only=True ) )\n",
    "    print( sg.to_list( verbose=True, keyword_only=False ) ) # not working\n",
    "    print( sg.to_list( verbose=False, keyword_only=True ) )\n",
    "    print( sg.to_list( verbose=False, keyword_only=False ) )\n",
    "    \n",
    "    #print( sgl.show() )\n",
    "    #print( sgr.show() )\n",
    "#     print( sgE.show() )\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SANDBOX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.power( 0.7, 1/2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rst = sorted( te.score.items(), key=lambda it: -1* ( it[1].cohesion_r * it[1].cohesion_l ) )\n",
    "tmp2 = open(\"_tmp/_dummy_corpus_score.txt\", 'w', encoding=\"utf-8\")\n",
    "pp = pprint.PrettyPrinter(indent=4, stream=tmp2)\n",
    "pp.pprint( [ ( r[0], r[1].freq , r[1].cohesion_r, r[1].cohesion_l ) for r in rst ] )\n",
    "\n",
    "print(\"ending\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "te.unigram_counter.get(\"枸\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "c = Counter(\"huihihu ihuihi\") \n",
    "d = Counter(\"werwwqweqriiiiiiiiiiii\")\n",
    "c.update(d)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "97 / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"c\" in {'a':1, 'b':2}.keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict( [('a', 1), ('b', 2)] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def entropy( p ):\n",
    "    return -1 * p * math.log2( p )\n",
    "entropy( 100 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"123456789\"[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import psutil\n",
    "import sys\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_available_memory():\n",
    "    \"\"\"It returns remained memory as percentage\"\"\"\n",
    "\n",
    "    mem = psutil.virtual_memory()\n",
    "    return 100 * mem.available / (mem.total)\n",
    "\n",
    "def get_process_memory():\n",
    "    \"\"\"It returns the memory usage of current process\"\"\"\n",
    "    \n",
    "    process = psutil.Process(os.getpid())\n",
    "    return process.memory_info().rss / (1024 ** 3)\n",
    "\n",
    "\n",
    "sys.stdout.write('\\rtraining ... (%d in %d sents) use memory %.3f Gb' % (100, 100, get_process_memory()))\n",
    "print('\\rtraining ... (%d in %d sents) use memory %.3f Gb' % (100, 100, get_process_memory()))\n",
    "print('\\rtraining was done. used memory %.3f Gb' % (get_process_memory()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "rg = re.compile(\"{0}\\d+?{0}\".format(\"%\") )\n",
    "# rg = re.compile(\"\\d+?\" )\n",
    "txt = \"朮破古紙半%1%生半炒骨碎%12%補穿山甲桑%8%灰炒爲珠生\"\n",
    "print( re.findall( rg, txt) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for x in Counter(['a', 'a', 'b']):\n",
    "    print(x)\n",
    "\n",
    "from itertools import chain, combinations\n",
    "k = Counter(['a', 'a', 'b', 'c'])\n",
    "combinations( k.keys(), 2) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = ['\\\\', '|', '/', '-']\n",
    "for i in range(0, 1000):\n",
    "    sys.stdout.write(\"\\r# {}\".format( k[i % 4]) )\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
